{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Text generation by Markov chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov chain is a probabalistic model in which the probability of each event depends only on the state attained in the previous event. [markovify](https://github.com/jsvine/markovify) is a library for text generation by Markov chain.\n",
    "\n",
    "Use \"pip install markovify\" to install markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download [Dataset.csv](https://drive.google.com/file/d/1raxIkJZ4lMTvgTB8eYjxu4Ux8aNL-x0s/view?usp=sharing) composed of sarcastic and serious headlines for the news. The csv-file consists of two columns. \"headline\" column contains texts of headlines. \"is_sarcastic\" column contain 0 if the hiadline is serious and 1 otherwise.\n",
    "\n",
    "Read dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11724 sarcastic texts in Dataset.csv\n",
      "Found 14985 serious texts in Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "texts_serious = [] # list of serious hiadline texts\n",
    "texts_sarcastic = [] # list of sarcastic hiadline texts\n",
    "with open('Dataset.csv', encoding='utf-8') as f:\n",
    "    # Rread csv-file by DictReader from csv library\n",
    "    reader = csv.DictReader(f) \n",
    "    for line in reader:\n",
    "        # read texts of headline\n",
    "        headline = line['headline'].strip()\n",
    "        # read sarcasticity of headline\n",
    "        is_sarcastic = int(line['is_sarcastic'].strip())\n",
    "        if is_sarcastic:\n",
    "            texts_sarcastic.append(headline)\n",
    "        else:\n",
    "            texts_serious.append(headline)\n",
    "print('Found {} sarcastic texts in Dataset.csv'.format(len(texts_sarcastic)))\n",
    "print('Found {} serious texts in Dataset.csv'.format(len(texts_serious)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge headlines into one text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_0 = '\\n'.join(texts_serious)\n",
    "text_1 = '\\n'.join(texts_sarcastic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Markov chain model for serious headlines generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "serious_text_model = markovify.NewlineText(texts_serious, state_size = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 20 serious headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this simple reform would get a little disappointing\n",
      "britain to send elite army unit to investigate post-election surge in the minority\n",
      "7 styling tricks to make your meetings more positive leader\n",
      "spanish government threatens to fire bad teachers\n",
      "the good life in prison\n",
      "simone biles pulls the perfect day in office. that matters.\n",
      "huffpost rise: what you need to have a bias against gay men in the shadow of a tiger looks like: faith instead of arresting panhandlers, albuquerque's giving them jobs\n",
      "demi lovato and wilmer valderrama decide to give book on earth\n",
      "alligator and python locked in death duel on golf course from the iran deal\n",
      "watch dirt bikers get a little disappointing\n",
      "how many women does it mean when we talk about abortion\n",
      "pelosi throws cold water on gay marriage and families\n",
      "bayer offers to buy clippers for $2 billion\n",
      "obama takes shots at bank drive-thru\n",
      "who needs 10 fps when you realize someone you thought was a baby with gorgeous photo\n",
      "chuck grassley introduces donald trump is growing\n",
      "who's to blame for the publishing industry?\n",
      "face it: let's talk about roger ailes\n",
      "i used an app to buy gifts, facebook responds with presents\n",
      "these are the spooky, yet cute trend of bad celebrity health advice\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "for i in range(20):\n",
    "    print(serious_text_model.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model and generate 20 sarcastic headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experts advise against throwing laptop across office even though it's what prince would have tipped gun control debate\n",
      "rich white people away\n",
      "government-publications enthusiast makes pilgrimage to facebook headquarters to lay readers\n",
      "google unveils new self-defense vase for smashing onto head of illinois nepotist party\n",
      "rat fancy magazine fails to reach climax\n",
      "tv show has another season in her office before leaving\n",
      "obama a little ad space to host 1998 ill-will games\n",
      "time traveler from the strokes accused of wartime non-profiteering\n",
      "either jay leno reconsiders retirement after georgia woman sets google alert for kevin costner\n",
      "hire of local moron gives nation hope for saving planet lies with people who save napkins from takeout order presumes man eats anything like human being terrific news for grover cleveland fans\n",
      "wheelchair-bound student would have been proud of unsuccessful child\n",
      "sxsw as cool and as real as it slowly lowers itself into warm arctic water\n",
      "nation demands more slow-motion footage of high tensile power lines\n",
      "guy from that one chinese place closes\n",
      "nation admits it can't do much for grandma this summer\n",
      "frantic john kerry poses as fox news covers spring break pretty well\n",
      "report: morbid curiosity now accounts for 38% of economic growth\n",
      "area man stops self after eating 20-piece spicy angel wings\n",
      "mpaa unveils rating system based on marriage counselor's recommendation\n",
      "surgery required for new nets\n"
     ]
    }
   ],
   "source": [
    "sarcastic_text_model = markovify.NewlineText(texts_sarcastic, state_size = 2)\n",
    "for i in range(20):\n",
    "    print(sarcastic_text_model.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation by Variation Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of tutorial based on [Text generation with a Variational Autoencoder](https://nicgian.github.io/text-generation-vae/) article. For sentence generation we use Variational Autoencoder (VAE) neural network model that is an extension seq2seq model. Originally VAE was described in [Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf) paper. The idea behind Variational Autoencoder is that we impose predefined disribution (e.g., normal distribution) on the latent state formed by encoder. On the one hand this restriction alow us to sample random vectors from normal distribution and generate arbitrary sentences. On the othe hand this restriction form very dense well differentiated space without holes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "from keras.layers import Bidirectional, Dense, Embedding, \\\n",
    "Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "from scipy import spatial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import random\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dowload [GloVe](http://nlp.stanford.edu/data/glove.6B.zip) pretrained word embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 15 # Max text length in tokens\n",
    "\n",
    "MAX_NB_WORDS = 20000 # Max words in dictionary\n",
    "EMBEDDING_DIM = 50 # Dimensionality of GloVe vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip glove.6b.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create sentence tokenizer and two dictionaries: word_to_id and id_to_word\n",
    "\n",
    "For tokenisation we use [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) from keras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize( lang,MAX_SEQUENCE_LENGTH):\n",
    "        # lang = list of sentences in a language\n",
    "        word_to_id={}\n",
    "        id_to_word={}\n",
    "        # print(len(lang), \"example sentence: {}\".format(lang[0]))\n",
    "        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='',oov_token='<OOV>')#, \n",
    "        lang_tokenizer.fit_on_texts(lang)\n",
    "        print('nb',lang_tokenizer.num_words)\n",
    "        ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) \n",
    "        ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)\n",
    "        tensor = lang_tokenizer.texts_to_sequences(lang) \n",
    "\n",
    "        ## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences \n",
    "        ## and pads the sequences to match the longest sequences in the given input\n",
    "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "        \n",
    "        for word, i in lang_tokenizer.word_index.items():\n",
    "            word_to_id[word]=i\n",
    "            id_to_word[i]=word\n",
    "            \n",
    "        \n",
    "        return tensor, lang_tokenizer,word_to_id,id_to_word\n",
    "    \n",
    "#or ##########################    \n",
    "texts = texts_serious + texts_sarcastic\n",
    "tokenizer = Tokenizer(MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_to_id = tokenizer.word_index \n",
    "id_to_word = {v: k for k, v in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize sarcastic texts and create tensor composed of tokens indexes. If a sentence shorter than MAX_SEQUENCE_LENGTH we pad it. If a sentence longer than MAX_SEQUENCE_LENGTH we cut it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data tensor shape: (11724, 15)\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts_sarcastic)\n",
    "data_1 = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Data tensor shape:', data_1.shape)\n",
    "NB_WORDS = (min(tokenizer.num_words, len(word_to_id)) + 1 ) #+1 for zero padding\n",
    "data_1_val = data_1[-1440:] #select 6000 sentences as validation data\n",
    "data_1 = data_1[:-1440]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tensor, lang_tokenizer,word_to_id,id_to_word=tokenize(texts_sarcastic,MAX_SEQUENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_WORDS = (len(word_to_id)) + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define batch generator to train a neural network\n",
    "\n",
    "For padding sentences to max length we use [pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sent_generator(TRAIN_DATA_FILE, batchsize):\n",
    "#     # Create iterator that reads dataset file batch by batch \n",
    "#     reader = pd.read_csv(TRAIN_DATA_FILE, chunksize=batchsize, iterator=True)\n",
    "#     for batch in reader:\n",
    "#         # Read a column that contains headlines\n",
    "#         #text=reader.headline.str.strip().tolist()\n",
    "#         text= batch['headline'].str.strip().tolist()\n",
    "#         # Tokenize texts and create padded tensor composed of tokens indexes\n",
    "        \n",
    "#         tensor, lang_tokenizer,word_to_id,id_to_word=tokenize(text,MAX_SEQUENCE_LENGTH)\n",
    "        \n",
    "#         # Return input-target pairs\n",
    "#         yield [tensor, tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_generator(TRAIN_DATA_FILE, batchsize):\n",
    "    # Create iterator that reads dataset file batch by batch \n",
    "    #your code here\n",
    "    reader = pd.read_csv(TRAIN_DATA_FILE, chunksize=batchsize, iterator=True)\n",
    "    for df in reader:\n",
    "        # Read a column that contains headlines\n",
    "        headlines = df['headline'].str.strip().tolist()\n",
    "        # Tokenize texts and create padded tensor composed of tokens indexes\n",
    "        sequences = tokenizer.texts_to_sequences(headlines)\n",
    "        data_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        # Return input-target pairs\n",
    "        yield [data_train, data_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pretrained GloVe vectors described in [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder='C:/Users/User/PycharmProjects/yandex/NLP class/Y-Data-NLP/Assignment 2/glove.6B/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(root_folder+'glove.6B.50d.txt', encoding='utf-8') as f:\n",
    "    # read rows from file line by line\n",
    "    for line in f:\n",
    "        #print(line)\n",
    "        values = line.split()\n",
    "        word = values[0] # Get word\n",
    "        #print(word)\n",
    "        coefs = np.asarray(values[1:], dtype='float32') # Get elements of word's vector\n",
    "        #print(coefs)\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create matrix from embedding vectors. Any row of the matrix is a word's vector. We get words from the dictionary word_to_id defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #NB_WORDS=len(embeddings_index)\n",
    "# glove_embedding_matrix = np.zeros((NB_WORDS, EMBEDDING_DIM)) # Create empty matrix (max number of tokens, dimension of the embedding vectors)\n",
    "# for word, i in word_to_id.items():\n",
    "#     if i < NB_WORDS:\n",
    "#         if embedding_vector is not None:\n",
    "#             # words not found in embedding index will be the word embedding of 'unk'.\n",
    "#             glove_embedding_matrix[i] = embedding_vector\n",
    "#         else:\n",
    "#             glove_embedding_matrix[i] = embeddings_index.get('unk')\n",
    "\n",
    "# # compute number of words which there aren't in the GloVe vectors\n",
    "# print('Null word embeddings: %d' % np.sum(np.sum(glove_embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 1\n"
     ]
    }
   ],
   "source": [
    "glove_embedding_matrix = np.zeros((NB_WORDS, EMBEDDING_DIM))\n",
    "for word, i in word_to_id.items():\n",
    "    if i < NB_WORDS:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be the word embedding of 'unk'.\n",
    "            glove_embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            glove_embedding_matrix[i] = embeddings_index.get('unk')\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(glove_embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters of the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_len = MAX_SEQUENCE_LENGTH\n",
    "emb_dim = EMBEDDING_DIM\n",
    "latent_dim = 32 # dimensionality of the hidden state in encoder and decoder RNN's\n",
    "intermediate_dim = 96 # dimensionality of variational space into which we map encoder's hidden state\n",
    "epsilon_std = 1.0 # standard deviation of gaussian noise\n",
    "act = ELU() # activation function of projection layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21590, 50)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder of the variational autoencoder. It based on bidirectional LSTM\n",
    "\n",
    "We use following layers: [Input](https://www.tensorflow.org/api_docs/python/tf/keras/Input), [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding), [Bidirectional](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional), [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM), [Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout), [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense), [ELU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ELU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Input layer fo the net. \n",
    "# Write an embedding layer for the input sequences of indexes. \n",
    "# Use pretrained word embeddings as a embedding layer\n",
    "# weights and don't update these weights\n",
    "from tensorflow.keras.layers import Embedding,Bidirectional,LSTM,Dense,Input\n",
    "x = Input(batch_shape=(None, max_len))\n",
    "x_embed = Embedding(NB_WORDS, emb_dim, weights=[glove_embedding_matrix],\n",
    "                            input_length=max_len, trainable=False)(x)\n",
    "# Bidirectional LSTM encoder\n",
    "\n",
    "h = Bidirectional(LSTM(intermediate_dim, return_sequences=False, recurrent_dropout=0.2), merge_mode='concat')(x_embed)\n",
    "h = Dropout(0.2)(h) # Dropout for the BiLSTM layer to avoid overfitting \n",
    "\n",
    "# Fully-connected layer to map encoder hidden state into variational space\n",
    "h = Dense(1, activation=\"linear\")(h)\n",
    "h = act(h)\n",
    "\n",
    "h = Dropout(0.2)(h) # Dropout for the fully-connected layer to avoid overfitting \n",
    "z_mean = Dense(latent_dim)(h) # Fully-connected layer to map variational space into means space \n",
    "z_log_var = Dense(latent_dim)(h) # Fully-connected layer to map variational space into standard deviations space "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mechanism for sampling hidden vectors from variational space\n",
    "\n",
    "To apply it to our model we use [Lambda](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    # Vectors from means space and standard deviations space respectively\n",
    "    z_mean, z_log_var = args\n",
    "    # Sample random vectors from normal distribution \n",
    "    # with mean=0 and std=epsilon_std\n",
    "    sample=np.random.normal(loc=0.0, scale=epsilon_std, size=(batch_size, latent_dim))\n",
    "    \n",
    "    \n",
    "    # Get new hidden state for decoder using vectors from means,\n",
    "    # standard deviations and normal random spaces\n",
    "    return z_mean + K.exp(z_log_var / 2) * sample\n",
    "\n",
    "# Get hidden states for the decoder\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define decoder of the autoencoder\n",
    "\n",
    "For this we use following layers: [RepeatVector](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RepeatVector), [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM), [TimeDistributed](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed), [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Repeat the hidden state vector to form input sequence for decoder\n",
    "# repeated_context = RepeatVector(max_len)\n",
    "# # Decoder LSTM\n",
    "# decoder_h = LSTM(intermediate_dim, return_sequences=True, recurrent_dropout=0.2)\n",
    "# # Layer for mapping from hidden satates space to the space of dimension equal to size of vocabulary\n",
    "# decoder_mean = TimeDistributed(Dense(NB_WORDS, activation='linear'))\n",
    "# # Generated sequence\n",
    "# h_decoded = decoder_h(repeated_context(z))\n",
    "# # Decode every time step vector of the decoded sequence into space of dimension equal to size of vocabulary\n",
    "# x_decoded_mean = decoder_mean(h_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Repeat the hidden state vector to form input sequence for decoder\n",
    "repeated_context = RepeatVector(max_len)\n",
    "# Decoder LSTM\n",
    "decoder_h = LSTM(intermediate_dim, return_sequences=True, recurrent_dropout=0.2)\n",
    "# Layer for mapping from hidden satates space to the space of dimension equal to size of vocabulary\n",
    "decoder_mean = tf.keras.layers.TimeDistributed(Dense(NB_WORDS, activation='linear'))\n",
    " \n",
    "# Generated sequence\n",
    "h_decoded = decoder_h(repeated_context(z))\n",
    "# Decode every time step vector of the decoded sequence into space of dimension equal to size of vocabulary\n",
    "x_decoded_mean = decoder_mean(h_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define layer for loss computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_loss(y_true, y_pred):\n",
    "    # Return tensor filled with ones with shape equal generated sequence shape\n",
    "    return K.zeros_like(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVariationalLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "        # Create tensor (batch_size, max_sequence_len) filled with ones to consider all elements of generated sequence \n",
    "        self.target_weights = tf.constant(np.ones((batch_size, max_len)),tf.float32)\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        # Get tensor with similar shape as x\n",
    "        labels = tf.cast(x, tf.int32)\n",
    "        # Compute sequence reconstruction loss\n",
    "        xent_loss = K.sum(tfa.seq2seq.sequence_loss(x_decoded_mean, labels,\n",
    "                                                    weights=self.target_weights,\n",
    "                                                    average_across_timesteps=False,\n",
    "                                                    average_across_batch=False), axis=-1)\n",
    "        # Compute KL-divergence as Variational loss \n",
    "        kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        # Composite loss (reconstruction loss + Variational loss)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0] # input sequence\n",
    "        x_decoded_mean = inputs[1] # reconstructed sequence\n",
    "        print(x.shape, x_decoded_mean.shape)\n",
    "        loss = self.vae_loss(x, x_decoded_mean) # Compute loss of the model\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # we don't use this output, but it has to have the correct shape\n",
    "        return K.ones_like(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assemble the model\n",
    "\n",
    "To define model we use [Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) from tensorflow.\n",
    "\n",
    "To train model employ [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 15) (64, 15, 21590)\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create custom layer for loss computing\n",
    "loss_layer = CustomVariationalLayer()([x, x_decoded_mean])\n",
    "\n",
    "vae = Model(x, [loss_layer])\n",
    "# Use Adam optimizer with learning rate = 0.01\n",
    "opt = Adam(learning_rate=0.01)\n",
    "vae.compile(optimizer='adam', loss=[zero_loss])\n",
    "# Show model structure\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint function to save states of our model during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_checkpoint(model_name):\n",
    "    filepath = model_name + \".h5\"\n",
    "    directory = os.path.dirname(filepath)\n",
    "    try:\n",
    "        # Check if directory exists\n",
    "        os.stat(directory)\n",
    "    except:\n",
    "        # If directory doesn't exist, create the directory\n",
    "        os.mkdir(directory)\n",
    "    # Save model states\n",
    "    checkpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=False)\n",
    "    return checkpointer\n",
    "\n",
    "# Create model checkpointer\n",
    "checkpointer = create_model_checkpoint(r'C:/Users/User/PycharmProjects/yandex/NLP class/Y-Data-NLP/Assignment 2/vae_seq2seq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model, test model after each epoch and save model's state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch:  0 -------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:830 train_function  *\n        return step_function(self, iterator)\n    c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:813 run_step  *\n        outputs = model.train_step(data)\n    c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:770 train_step  *\n        y_pred = self(x, training=True)\n    c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\base_layer.py:989 __call__  *\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\input_spec.py:197 assert_input_compatibility  *\n        raise ValueError('Layer ' + layer_name + ' expects ' +\n\n    ValueError: Layer model_1 expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, None) dtype=int32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, None) dtype=int32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-c04556fc6020>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     vae.fit_generator(sent_generator('Dataset.csv', batch_size),\n\u001b[0;32m      7\u001b[0m                       \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                       validation_data=(data_1_val, data_1_val))\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'vae_lstmFull32dim96hid.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1930\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1931\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1932\u001b[1;33m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1934\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1156\u001b[0m                 _r=1):\n\u001b[0;32m   1157\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1158\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1159\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1160\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    922\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3020\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m-> 3022\u001b[1;33m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[0;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3439\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[0;32m   3440\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[1;32m-> 3441\u001b[1;33m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[0;32m   3442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[0;32m   3361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m     graph_function = self._create_graph_function(\n\u001b[1;32m-> 3363\u001b[1;33m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[0;32m   3364\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3289\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:830 train_function  *\n        return step_function(self, iterator)\n    c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:813 run_step  *\n        outputs = model.train_step(data)\n    c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:770 train_step  *\n        y_pred = self(x, training=True)\n    c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\base_layer.py:989 __call__  *\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\input_spec.py:197 assert_input_compatibility  *\n        raise ValueError('Layer ' + layer_name + ' expects ' +\n\n    ValueError: Layer model_1 expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, None) dtype=int32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, None) dtype=int32>]\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 8 # number of epochs for model training\n",
    "n_steps = 28000 / batch_size # Number of steps per epoch\n",
    "for counter in range(nb_epoch):\n",
    "    print('-------epoch: ', counter, '-------')\n",
    "    # Train model. Test and save model after every epoch\n",
    "    vae.fit_generator(sent_generator('Dataset.csv', batch_size),\n",
    "                      steps_per_epoch=n_steps, epochs=1, callbacks=[checkpointer],\n",
    "                      validation_data=(data_1_val, data_1_val))\n",
    "vae.save(r'vae_lstmFull32dim96hid.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assemble encoder and decoder for sentence generation sampled from variational space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make separate encoder to encode input sentence\n",
    "encoder = Model(x, z_mean)\n",
    "# Input layer for decoder to decode vectors sampled from variational space \n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "# Apply LSTM to decode hidden vector into sequence\n",
    "_h_decoded = decoder_h(repeated_context(decoder_input))\n",
    "# Decode every time step vector of the decoded sequence into space of dimension equal to size of vocabulary\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "# Apply softmax to get most probable token\n",
    "_x_decoded_mean = Activation('softmax')(_x_decoded_mean)\n",
    "# Make decoderfor sempled sentences\n",
    "generator = Model(decoder_input, _x_decoded_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-a83398fd6c22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mindex2word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_to_id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Fit sentences from validation set into encoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msent_encoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_1_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# Decode encoded sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mx_test_reconstructed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_encoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "# Dictionary maps indexes to words\n",
    "index2word = {v: k for k, v in word_to_id.items()}\n",
    "# Fit sentences from validation set into encoder\n",
    "sent_encoded = encoder.predict(data_1_val, batch_size=16)\n",
    "# Decode encoded sentences\n",
    "x_test_reconstructed = generator.predict(sent_encoded)\n",
    "\n",
    "sent_idx = 400\n",
    "# Get words indexes with highest probability for the 500th sentence from validation set\n",
    "reconstructed_indexes = np.apply_along_axis(np.argmax, 1, x_test_reconstructed[sent_idx])\n",
    "# Map indexes of generated sentence to words\n",
    "word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
    "word_list = ' '.join([w for w in word_list if w])\n",
    "print(word_list)\n",
    "# Map indexes of input sentence to words\n",
    "original_sent = list(np.vectorize(index2word.get)(data_1_val[sent_idx]))\n",
    "original_sent = ' '.join([w for w in original_sent if w])\n",
    "print(original_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
